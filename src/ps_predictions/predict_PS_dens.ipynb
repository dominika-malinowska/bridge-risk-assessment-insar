{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885156f7",
   "metadata": {},
   "source": [
    "This script can be used to predict the pixel-wise density of the PS points for a given region. \n",
    "\n",
    "After the variables specifying the region of interest are set, the code downloads and pre-processes the input data:\n",
    "- the osm dataset that's used to create an infrastructure map\n",
    "- the long-term coherence.\n",
    "\n",
    "Then, it applies a pre-trained model (downloaded automatically from HuggingFace) to this inputs and generates prediction of PS density that are saved as a geotiff file.\n",
    "\n",
    "Note: the current version only works for regions of interest within one-by-one degree tile. Future updates will extend the functionality to remove this constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac6d89",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e9ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../..\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"src\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5473be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchgeo.datasets import (\n",
    "    stack_samples,\n",
    ")\n",
    "from torchgeo.samplers import GridGeoSampler\n",
    "\n",
    "from lightning.pytorch import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a87fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.find_repo import find_repo_root\n",
    "\n",
    "from ps_predictions.utils.data_download import calculate_area, download_osm\n",
    "\n",
    "from ps_predictions.utils.torchgeo_classes_def import PixelwiseRegressionTask\n",
    "\n",
    "from ps_predictions.utils.torchgeo_fun_to_read_input import (\n",
    "    scale_coh,\n",
    "    RasterDataset_imgs,\n",
    ")\n",
    "from ps_predictions.utils.prediction_plots import (\n",
    "    georreferenced_chip_generator,\n",
    "    merge_georeferenced_chips,\n",
    ")\n",
    "\n",
    "from ps_predictions.utils.infrastructure_map_generation import generate_infrastructure_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e1db53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if torch with cuda enabled and working\n",
    "# torch.zeros(1).cuda()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9321901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warning about number of warnings\n",
    "# See https://github.com/Lightning-AI/lightning/issues/10182\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc52f5",
   "metadata": {},
   "source": [
    "# Set-up region to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60af0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! FOR NOW IT ONLY WORKS IF THE REGION IS WITHIN ONE BY ONE DEGREE TILE (TO BE UPDATED IN FUTURE VERSIONS) !!!\n",
    "x = [18.53, 18.9]\n",
    "y = [50.05, 50.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d0a5d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 1321.90 km²\n"
     ]
    }
   ],
   "source": [
    "print(f\"Area: {calculate_area(x,y):.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7b0f8",
   "metadata": {},
   "source": [
    "# Set-up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where all data should be saved\n",
    "\n",
    "# Find the repo root by looking for a marker file (e.g., .git)\n",
    "repo_root = find_repo_root()\n",
    "\n",
    "# Define the path to the data\n",
    "root = os.path.join(repo_root, \"data\")\n",
    "folder_name = \"ps_prediction_with_ML_model\"\n",
    "experiment_name = \"experiment_slaskie\"\n",
    "\n",
    "path_prefix = os.path.join(root, folder_name)\n",
    "experiment_dir = os.path.join(path_prefix, experiment_name)\n",
    "\n",
    "# Path for saving results (such as model checkpoints)\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4504db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific OSM file to use (depending on the region of interest)\n",
    "# The more specific, i.e. the smaller the file, the faster the code will run\n",
    "filename_OSM = \"europe/poland/slaskie-latest.osm.pbf\"\n",
    "osm_pbf_location = os.path.join(experiment_dir, \"slaskie-latest.osm.pbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1139c",
   "metadata": {},
   "source": [
    "# Download and pre-process inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08771bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processed tile lat:50.05,50.5 lon:18.53,18.9\n"
     ]
    }
   ],
   "source": [
    "# Set variables required for infrastructure map generation and coherence download\n",
    "tags_high = {\"highway\": [\"motorway\", \"trunk\", \"primary\", \"secondary\"]}\n",
    "tags_build = {\"building\": [\"yes\"]}\n",
    "tags_rail = {\"railway\": [\"rail\"]}\n",
    "\n",
    "min_x = min(x)\n",
    "min_y = min(y)\n",
    "max_x = max(x)\n",
    "max_y = max(y)\n",
    "\n",
    "# Convert min_x and min_y to integers for use in formatting the latitude and longitude\n",
    "west = math.floor(min_x)\n",
    "south = math.floor(min_y)\n",
    "east = math.ceil(max_x)\n",
    "north = math.ceil(max_y)\n",
    "\n",
    "# Convert min_x and min_y to integers for use in formatting the latitude and longitude\n",
    "lon = int(min_x)\n",
    "lat = int(min_y)\n",
    "\n",
    "# Format the latitude and determine the season based on whether the latitude is positive or negative\n",
    "if south >= 0:\n",
    "    lat_f = \"N{:02d}\".format(south + 1)\n",
    "    season = \"summer\"\n",
    "else:\n",
    "    lat_f = \"S{:02d}\".format(abs(south + 1))\n",
    "    season = \"winter\"\n",
    "    \n",
    "# Format the longitude based on whether it is positive or negative\n",
    "if west >= 0:\n",
    "    lon_f = \"E{:03d}\".format(west)\n",
    "else:\n",
    "    lon_f = \"W{:03d}\".format(abs(west))\n",
    "    \n",
    "# Print the currently processed tile's coordinates\n",
    "print(\n",
    "    \"Currently processed tile lat:{},{} lon:{},{}\".format(min_y, max_y, min_x, max_x),\n",
    "    flush=True,\n",
    ")\n",
    "\n",
    "# Define bounds for the prediction tile\n",
    "bounds = (min_x, min_y, max_x, max_y)\n",
    "bounds_name = (int(min_x), int(min_y), int(max_x), int(max_y))\n",
    "bounds_ltcoh = (\n",
    "    math.floor(min_x),\n",
    "    math.floor(min_y),\n",
    "    math.ceil(max_x),\n",
    "    math.ceil(max_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b8ed5",
   "metadata": {},
   "source": [
    "## Download long-term coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a28766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Long-term coherence tile N51E018 is missing! Trying to download \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sentinel-1-global-coherence-earthbigdata/data/tiles/N51E018/N51E018_summer_vv_rho.tif to ../../data/testing_code_for_publishing3/experiment2_slaskie/N51E018/coh/N51E018_summer_vv_rho.tif\n",
      "Download of N51E018 tiles ended.\n",
      "Total execution time for downloading the long term cohrence file: 17.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start of long-term coherence section\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the path for the long-term coherence tile\n",
    "coh_tile_path = \"{coh_dir}/{lat}{lon}/coh/{lat}{lon}_{season}_vv_rho.tif\".format(\n",
    "    lat=lat_f, lon=lon_f, season=season, coh_dir=experiment_dir\n",
    ")\n",
    "\n",
    "# Check if the long-term coherence tile already exists\n",
    "# If not, try to download it\n",
    "if os.path.exists(coh_tile_path):\n",
    "    print(\n",
    "        \"Long-term coherence tile  {}{} is already downloaded.\".format(lat_f, lon_f),\n",
    "        flush=True,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"\\n Long-term coherence tile {}{} is missing! Trying to download \\n\".format(\n",
    "            lat_f, lon_f\n",
    "        ),\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        os.system(\n",
    "            f\"aws s3 cp s3://sentinel-1-global-coherence-earthbigdata/data/tiles/{lat_f}{lon_f}/\"\n",
    "            f\"{lat_f}{lon_f}_{season}_vv_rho.tif --no-sign-request \"\n",
    "            f\"{experiment_dir}/{lat_f}{lon_f}/coh/{lat_f}{lon_f}_{season}_vv_rho.tif\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(\n",
    "            \"{coh_dir}/{lat}{lon}/coh/{lat}{lon}_{season}_vv_rho.tif\".format(\n",
    "                lat=lat_f, lon=lon_f, season=season, coh_dir=experiment_dir\n",
    "            )\n",
    "        ):\n",
    "            print(\"Download of {}{} tiles ended.\".format(lat_f, lon_f))\n",
    "        else:\n",
    "            print(\"\\n Download of tile {}{} failed. \\n\".format(lat_f, lon_f))\n",
    "\n",
    "    except BaseException as error:\n",
    "        print(\n",
    "            \"There is a problem with the download of the file. Please see following line to identify the error.\"\n",
    "        )\n",
    "        print(\"An exception occurred: {}\".format(error))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An exception occurred: {}\".format(e))\n",
    "        \n",
    "# Calculate the total execution time for checking the long-term coherence file\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\n",
    "    f\"Total execution time for downloading the long term cohrence file: {total_time:.2f} seconds\",\n",
    "    flush=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4f076",
   "metadata": {},
   "source": [
    "## Generate infrastructure map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432ca53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading europe/poland/slaskie-latest.osm.pbf...\n",
      "Progress: 100.0%\n",
      "Download complete! File saved as /mnt/c/Users/Dominika/Desktop/Code_for_AWS_data/bridge-risk-assessment-insar/data/testing_code_for_publishing3/experiment2_slaskie/slaskie-latest.osm.pbf\n"
     ]
    }
   ],
   "source": [
    "# Download required OSM data\n",
    "download_osm(filename_OSM, osm_pbf_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5332452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory for the infrastructure map raster\n",
    "infrmap_raster_dir = os.path.join(\n",
    "    experiment_dir,\n",
    "    \"{}{}\".format(lat_f, lon_f),\n",
    "    \"infr_map\",\n",
    "    \"{}{}_infrs_map.tif\".format(lat_f, lon_f),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ba9952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Dominika/Desktop/Code_for_AWS_data/bridge-risk-assessment-insar/data/testing_code_for_publishing3/experiment2_slaskie/N51E018/N51E018.o5m started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Dominika/Desktop/Code_for_AWS_data/bridge-risk-assessment-insar/data/testing_code_for_publishing3/experiment2_slaskie/N51E018/N51E018.o5m finished!\n",
      "Converting of o5m for N51E018 is done\n",
      "1637830 20277 12414\n",
      "Time for df creation: 131.27 seconds\n",
      "Time for raster generation: 230.43 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check if the infrastructure map already exists\n",
    "if os.path.exists(infrmap_raster_dir):\n",
    "    print(\n",
    "        \"Infrastructure map {}{} is already generated.\".format(lat_f, lon_f),\n",
    "        flush=True,\n",
    "    )\n",
    "# If the infrastructure map does not exist, start the generation process\n",
    "else:\n",
    "    generate_infrastructure_map(\n",
    "        lat_f,\n",
    "        lon_f,\n",
    "        min_x,\n",
    "        min_y,\n",
    "        max_x,\n",
    "        max_y,\n",
    "        experiment_dir,\n",
    "        osm_pbf_location,\n",
    "        infrmap_raster_dir,\n",
    "        osm_convert_path = \"/usr/bin/osmconvert\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f5d7d",
   "metadata": {},
   "source": [
    "# Download and set-up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbba90",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4b58da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b709072b63d46198672bed63e976728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seed_54321_20231214-203338.ckpt:   0%|          | 0.00/293M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint downloaded to: /mnt/c/Users/Dominika/Desktop/Code_for_AWS_data/bridge-risk-assessment-insar/data/testing_code_for_publishing3/best_models_15122023/seed_54321_20231214-203338.ckpt\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(path_prefix, \"best_models_15122023\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Download the specific checkpoint file\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"dominika-malinowska/ps_predictions\",\n",
    "    filename=\"seed_54321_20231214-203338.ckpt\",\n",
    "    local_dir=model_dir  # download to this folder\n",
    ")\n",
    "\n",
    "print(f\"Checkpoint downloaded to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1442595",
   "metadata": {},
   "source": [
    "## Set-up model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bab9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant parameters\n",
    "# Those shouldn't be changed if you want to use the model in the same way as in the paper\n",
    "in_channels = 2  # Number of channels in input image\n",
    "ignore_index = None  # Optional integer class index to ignore in the loss and metrics\n",
    "lr_patience = 10  # Patience for learning rate scheduler\n",
    "min_delta_lr = 0.01\n",
    "patience = 15  # Patience for early stopping\n",
    "min_delta = (\n",
    "    0.01  # when change in val_loss less than that for appropriate number of epochs\n",
    ")\n",
    "# early stopping will be triggered\n",
    "max_epochs = 300  # maximum number of epochs\n",
    "param_opts = [\n",
    "    [16, 64, None, \"resnet34\", None, \"mae\", 0.00776601, 0.9, 0.99, 0.01],\n",
    "]\n",
    "patch_size = param_opts[0][1]\n",
    "batch_size = param_opts[0][0]\n",
    "# Select overlap parameter for merging patches in prediction\n",
    "overlap = 8\n",
    "\n",
    "# GPU settings\n",
    "gpu_id = 0\n",
    "device = torch.device(f\"cuda:{gpu_id}\")\n",
    "num_dataloader_workers = 0\n",
    "\n",
    "param_const = [\n",
    "    in_channels,\n",
    "    ignore_index,\n",
    "    lr_patience,\n",
    "    min_delta_lr,\n",
    "    patience,\n",
    "    min_delta,\n",
    "    max_epochs,\n",
    "    gpu_id,\n",
    "    device,\n",
    "    num_dataloader_workers,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfde663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the PS prediction raster\n",
    "file_name = os.path.join(\n",
    "    experiment_dir, \"{}{}\".format(lat_f, lon_f), \"{}{}_PS_pred.tif\".format(lat_f, lon_f)\n",
    ")\n",
    "\n",
    "# Create the directory for storing outputs if it doesn't exist\n",
    "dir_coord = os.path.join(experiment_dir, \"{}{}\".format(lat_f, lon_f))\n",
    "if not os.path.exists(dir_coord):\n",
    "    os.makedirs(dir_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac9181",
   "metadata": {},
   "source": [
    "## Read trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89bbfc8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Read trained model\n",
    "model = PixelwiseRegressionTask.load_from_checkpoint(\n",
    "    checkpoint_path=os.path.join(\n",
    "        model_dir, \"seed_54321_20231214-203338.ckpt\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e507db56",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 54321\n"
     ]
    }
   ],
   "source": [
    "# Set-up seed\n",
    "seed = 54321\n",
    "seed_everything(seed, workers=True)\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f933f9e",
   "metadata": {},
   "source": [
    "# Prepare input data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7adeecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting RasterDataset_imgs res from 0.0008333333333333373 to 0.00083333333\n"
     ]
    }
   ],
   "source": [
    "# Create RasterDataset objects for coherence and infrastructure map images\n",
    "imgs_coh = RasterDataset_imgs(\n",
    "    paths=os.path.join(\n",
    "        experiment_dir,\n",
    "        \"{}{}\".format(lat_f, lon_f),\n",
    "        \"coh\",\n",
    "    ),\n",
    "    transforms=scale_coh,\n",
    ")\n",
    "imgs_infrmap = RasterDataset_imgs(\n",
    "    paths=os.path.join(experiment_dir, \"{}{}\".format(lat_f, lon_f), \"infr_map\")\n",
    ")\n",
    "# Combine the two RasterDataset objects\n",
    "imgs_input = imgs_coh & imgs_infrmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3670c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset to predict\n",
    "dataset_to_pred = imgs_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36db5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GridGeoSampler object for the dataset\n",
    "sampler_pred = GridGeoSampler(dataset_to_pred, patch_size, patch_size - overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c4de09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "dataloader_interf = DataLoader(\n",
    "    dataset=dataset_to_pred,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler_pred,\n",
    "    num_workers=0,\n",
    "    collate_fn=stack_samples,\n",
    "    generator=generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0b2c6",
   "metadata": {},
   "source": [
    "# Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a62e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken to predict was:  7.7916483879089355\n",
      "The time taken to generate a georrefenced image and save it was:  0.10680079460144043\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Generate geotiff\n",
    "# Get the pixel size and CRS of the dataset\n",
    "pixel_size = dataset_to_pred.res\n",
    "crs = dataset_to_pred.crs.to_epsg()\n",
    "\n",
    "\n",
    "# Generate prediction chips\n",
    "start = time.time()  # Start measuring the time\n",
    "chips_generator = georreferenced_chip_generator(\n",
    "    dataloader_interf, model, crs, pixel_size\n",
    ")\n",
    "print(\"The time taken to predict was: \", time.time() - start, flush=True)\n",
    "\n",
    "# Save the prediction chips as a geotiff\n",
    "start = time.time()  # Start measuring the time\n",
    "merge_georeferenced_chips(chips_generator, file_name, overlap, bounds)\n",
    "print(\n",
    "    \"The time taken to generate a georrefenced image and save it was: \",\n",
    "    time.time() - start,\n",
    "    flush=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65257e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0791678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "ps-predictions-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
